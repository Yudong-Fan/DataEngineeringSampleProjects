{"cells":[{"cell_type":"markdown","source":["# Import libraries"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d6de8817-fdae-436f-b278-6034c6540701"}}},{"cell_type":"code","source":["from pyais import decode, IterMessages\nimport math\nimport numpy as np\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import Row\nfrom pyspark.sql.types import *\nimport pyspark.sql.functions as f\nfrom pyspark.sql.functions import col, input_file_name, when \nfrom pyspark.sql.window import Window"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"564f3541-2b33-456f-8889-44b3a94aaf2a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# https://github.com/M0r13n/pyais\n# the library for decoding AIS messages, uncoment the nextline for downloading\n# %pip install pyais\nsc = spark.sparkContext"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b2877184-531a-4745-b128-babf735647a4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Functions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0fbea056-5b53-4d4a-8308-b2cc670b613f"}}},{"cell_type":"code","source":["# decoding raw messages\ndef get_decoded(row):\n    decoded = {}\n    try:\n        decoded = decode(row[0]).asdict()\n        decoded['filename'] = row[1]\n    except Exception:\n        pass\n    # cast 'status' from pyais object to float for spark to handle the type     \n    try:\n        decoded['status'] = float(decoded['status'])\n    except (TypeError, KeyError) as error:\n        pass\n    # return a dictionary of the decoded message     \n    return decoded"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b9252be2-12e0-441a-a501-877c41073c00"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# select desired features\ndef get_features(row):\n    schema_list = list(row)\n    # if the message contains undesired feature, remove it     \n    for i in schema_list:\n        if i not in feature_list:\n            row.pop(i, None)\n    # if the message doesn't contain desired features, add it and initialize to None\n    for j in feature_list:\n        if j not in schema_list:\n            row[j] = None\n    # create a new dictionary which stores keys following the order in the feature_list\n    new_row = {}\n    for k in feature_list:\n        new_row[k] = row[k]\n    # convert the dictionary to a Row object and return it\n    return Row(**new_row)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1ab8e19d-912d-4e3b-b4df-984130919996"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# encode eta (estimate time of arrival)\ndef get_eta(row):\n    eta = ''\n    if row[1].month != 0 and row[1].day != 0 and row[1].hour != 0 and row[1].minute != 0:\n        eta = '{m}-{d} {hour}:{minute}'.format(m = str(int(row[1].month)).zfill(2), d = str(int(row[1].day)).zfill(2), hour = str(int(row[1].hour)).zfill(2), minute = str(int(row[1].minute)).zfill(2))\n    else:\n        eta = 'nan'\n    return (row[0], eta)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e002bdfc-b2e9-416a-bb7a-59cc61b9dc40"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# parse the filename and generate timestamp \ndef get_timestamp(row):\n    year = row.filename[29:33]\n    month = row.filename[34:36]\n    day = row.filename[37:39]\n    hour = row.filename[40:42]\n    minute = row.filename[43:45]\n    ts = '{yy}-{mm}-{dd} {hour}:{minute}'.format(yy = year, mm = str(month).zfill(2), dd = str(day).zfill(2), hour = str(hour).zfill(2), minute = str(minute).zfill(2)) \n    return (row, ts)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bb68ede6-d28e-4e45-897b-d5eb753d5196"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def read_1hour_pipeline(files, year, month, d):\n    write_out_path = '/mnt/lsde/group12/input/{yy}-{mm}-{dd}/'.format(yy = year, mm = str(month).zfill(2), dd = str(d).zfill(2))\n    \n    df = spark.read.format(\"csv\").option(\"header\", \"false\").option(\"delimiter\", \",\").option(\"inferschema\",\"false\").load(files)  \n    df = df.withColumn('filename', input_file_name())\n    # replace nan with \"\"\n    df_no_null = df.na.fill(\"\")\n    # turn rows into parsable strings for the decoder, rdd -> (raw_message, filename)\n    stream = df_no_null.rdd.map(lambda x: ((x[0] + \",\" + x[1] + \",\" + x[2] + \",\" + x[3] + \",\" + x[4] + \",\" + x[5] + \",\" + x[6]), x[7]) )\n    # get decoded messages with filename encoded, rdd -> (dict(messages))     \n    df_decoded = stream.map(get_decoded)\n    # get desired features, rdd -> (Row)\n    df_good_features = df_decoded.map(get_features).toDF(schema)\n    # select desired message types     \n    df_good_types = df_good_features.filter(col('msg_type').isin(type_list))\n    # select desired sea area (north atlantic)     \n    df_good_area = df_good_types.filter((col('lat') >= -0.936) & (col('lat') <= 68.6387) & (col('lon') >= -98.0539) & (col('lon') <= 12.0059)) \n    # replace null in month, day, hour, minute for eta generation\n    df_pruned = df_good_area.na.fill(0, eta_list)\n    # encode index to rdd, rdd -> (index, row)\n    df_pruned_with_index = df_pruned.rdd.zipWithIndex().map(lambda row: (row[1], row[0]))\n    # get eta, rdd -> (index, eta)\n    eta = df_pruned_with_index.map(get_eta)\n    # join df_pruned and eta on index, rdd after join -> (index, (row, eda)), rdd after map -> (Row)\n    with_eta = df_pruned_with_index.join(eta).map(lambda row: Row(**row[1][0].asDict(), eda = row[1][1]))\n    # get timestamps for entries, rdd -> (Row, timestamp) -> (Row)\n    with_timestamp = with_eta.map(get_timestamp).map(lambda row: Row(**row[0].asDict(), timestamp = row[1])).toDF(schema2)\n    # drop unneeded columns and ready to write out\n    final_df = with_timestamp.drop('month', 'day', 'hour', 'minute', 'filename')\n    # write to prefix:/input in parquet format\n    if write_out_path[24:] not in [file.name for file in dbutils.fs.ls('/mnt/lsde/group12/input/')]:\n        final_df.write.format(\"parquet\").save(write_out_path)\n    else:\n        final_df.write.mode('append').format(\"parquet\").save(write_out_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6dd178b5-a66d-4c82-8d52-5c65fa8bdcd4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_filepaths_1hour(year, month, day, hour):\n    try:\n        # handle empty directories\n        paths_list = dbutils.fs.ls('/mnt/lsde/datasets/ais2/{y}/{m}/{d}/'.format(y = str(year).zfill(2), m = str(month).zfill(2), d = str(day).zfill(2)))\n    except (Exception):\n        return []\n    existing_paths = []\n    for i in paths_list:\n        if i.path[37:39] == str(day).zfill(2) and i.path[40:42] == str(hour).zfill(2):\n            existing_paths.append(i.path[5:])\n    return existing_paths   "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"16c3a17b-ba41-4b5b-80f3-454b4effb196"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Processing data for 1 day at a time will cause a significant delay in processing time, thus deprecated\n# def get_filepaths_1day(year, month, day):\n#     paths_list = dbutils.fs.ls('/mnt/lsde/datasets/ais2/{y}/{m}/{d}/'.format(y = str(year).zfill(2), m = str(month).zfill(2), d = str(day).zfill(2)))\n#     existing_paths = []\n#     for i in paths_list:\n#         if i.path[37:39] == str(day).zfill(2):\n#             existing_paths.append(i.path[5:])\n#     return existing_paths   "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bbb86b00-432b-4f7e-b0f8-b31ef5b48d6d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def extract_data(year, month, day_head, day_tail):\n    # date\n    for d in range(day_head, day_tail):\n        # hour\n        for h in range(0, 24):\n            files = get_filepaths_1hour(year, month, d, h)\n            # skip incomplete or missing file paths              \n            try:\n                read_1hour_pipeline(files, year, month, d)\n            except (Exception):\n                continue"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"95e726c5-a248-4433-8f97-4a769bad689f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Query"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4b249653-dac4-4c62-89b5-34d5723c53ce"}}},{"cell_type":"code","source":["eta_list = ['month', 'day', 'hour', 'minute']\ntype_list = [1,2,3,5,18,19,24,27]\nfeature_list = ['mmsi', 'msg_type', 'imo', 'callsign', 'status', \n            'shipname', 'ship_type', 'lon', 'lat', 'to_bow',\n            'to_stern', 'to_port', 'to_starboard', 'heading', \n            'course', 'speed', 'month', 'day', 'hour', 'minute', \n            'draught', 'destination', 'radio', 'filename']\n\nschema = StructType([StructField('mmsi', LongType(), True), StructField('msg_type', LongType(), True), StructField('imo', LongType(), True),\n                    StructField('callsign', StringType(), True), StructField('status', FloatType(), True), StructField('shipname', StringType(), True),\n                    StructField('ship_type', LongType(), True), StructField('lon', FloatType(), True), StructField('lat', FloatType(), True),\n                    StructField('to_bow', LongType(), True), StructField('to_stern', LongType(), True), StructField('to_port', LongType(), True),\n                    StructField('to_starboard', LongType(), True), StructField('heading', LongType(), True), StructField('course', FloatType(), True),\n                    StructField('speed', FloatType(), True), StructField('month', LongType(), True), StructField('day', LongType(), True),\n                    StructField('hour', LongType(), True), StructField('minute', LongType(), True), StructField('draught', FloatType(), True),\n                    StructField('destination', StringType(), True), StructField('radio', LongType(), True), StructField('filename', StringType(), True)])\n\nschema2 = StructType([StructField('mmsi', LongType(), True), StructField('msg_type', LongType(), True), StructField('imo', LongType(), True),\n                    StructField('callsign', StringType(), True), StructField('status', FloatType(), True), StructField('shipname', StringType(), True),\n                    StructField('ship_type', LongType(), True), StructField('lon', FloatType(), True), StructField('lat', FloatType(), True),\n                    StructField('to_bow', LongType(), True), StructField('to_stern', LongType(), True), StructField('to_port', LongType(), True),\n                    StructField('to_starboard', LongType(), True), StructField('heading', LongType(), True), StructField('course', FloatType(), True),\n                    StructField('speed', FloatType(), True), StructField('month', LongType(), True), StructField('day', LongType(), True),\n                    StructField('hour', LongType(), True), StructField('minute', LongType(), True), StructField('draught', FloatType(), True),\n                    StructField('destination', StringType(), True), StructField('radio', LongType(), True), StructField('filename', StringType(), True),\n                    StructField('eta', StringType(), True), StructField('timestamp', StringType(), True)])\n\n# modify the time range for extracting data\nyear = 2016\nmonth = 7\nday_head = 0     #includes\nday_tail = 32     #excludes\n\nextract_data(year, month, day_head, day_tail)\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c56c299-c1f7-432c-a578-5e6bdd842af4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"S4-dataExtraction","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":4105269847535003}},"nbformat":4,"nbformat_minor":0}
