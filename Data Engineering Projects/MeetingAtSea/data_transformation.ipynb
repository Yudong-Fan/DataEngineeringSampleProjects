{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"947b817b-bbe8-40f5-bd51-b37acc1cc4af","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Python interpreter will be restarted.\n","Collecting pygeohash\n","  Using cached pygeohash-1.2.0-py2.py3-none-any.whl\n","Installing collected packages: pygeohash\n","Successfully installed pygeohash-1.2.0\n","Python interpreter will be restarted.\n","Python interpreter will be restarted.\n","Collecting haversine\n","  Using cached haversine-2.7.0-py2.py3-none-any.whl (6.9 kB)\n","Installing collected packages: haversine\n","Successfully installed haversine-2.7.0\n","Python interpreter will be restarted.\n","</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Python interpreter will be restarted.\nCollecting pygeohash\n  Using cached pygeohash-1.2.0-py2.py3-none-any.whl\nInstalling collected packages: pygeohash\nSuccessfully installed pygeohash-1.2.0\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nCollecting haversine\n  Using cached haversine-2.7.0-py2.py3-none-any.whl (6.9 kB)\nInstalling collected packages: haversine\nSuccessfully installed haversine-2.7.0\nPython interpreter will be restarted.\n</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["%pip install pygeohash\n","%pip install haversine"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"19ee47cc-0729-4ab9-9e2e-15706f59713e","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["import sys\n","import math\n","import pandas as pd\n","import os\n","# from pyais import decode, IterMessages\n","from pyspark.sql import SparkSession\n","import pyspark\n","import pandas as pd\n","import numpy as np\n","from pyspark.rdd import RDD\n","from pyspark.sql import Row\n","from pyspark.sql import DataFrame\n","from pyspark.sql import SparkSession\n","from pyspark.sql import SQLContext\n","from pyspark.sql import functions\n","from pyspark.sql.functions import *\n","from pyspark.sql.types import *\n","import seaborn as sns\n","import itertools\n","from itertools import *\n","from haversine import haversine, Unit\n","import sys\n","from pyspark.sql import Window\n","import pyspark.sql.functions as F\n","import pygeohash\n","import re"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"3e6d5339-60d6-45ca-b1e2-2d09e642ff36","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["def Ship_position_reports(df):\n","    \"\"\"\n","    This function retuns the dataframe with dynamic ais messages\n","    \n","    \"\"\"\n"," \n","    dynamic_col = ['mmsi','msg_type','status','lon','lat','heading','course','speed','timestamp']\n","    #select the static messages\n","    msg_type_filter = col('msg_type').isin([1,2,3,18,27])\n","    AIS_Dynamic_MSG = df.select(dynamic_col).filter(msg_type_filter)\n","    AIS_Dynamic_MSG = AIS_Dynamic_MSG.filter(valid_course & valid_heading & valid_lon & valid_lat)\n","    \n","    Status_code = {0.0 : \"Under way using its engine\",\n","                1.0 : \"Anchored\",\n","                2.0: \"Not under command\",\n","                3.0 : \"Has restricted maneuverability\",\n","                4.0: \"Ship draught is limiting its movement\",\n","                5.0 : \"Moored\",\n","                6.0: \"Aground\",\n","                7.0: \"Engaged in fishing\",\n","                8.0: \"Under way sailing\",\n","                9.0: \"carrying dangerous goods\",\n","                10.0 : \"carrying dangerous goods\",\n","                11.0: \"Power-driven vessel towing astern\",\n","                12.0: \"Power-driven vessel pushing ahead\",\n","                13.0: \"(Reserved for future use)\",\n","                14.0: \"Emergency\",\n","                15.0: \"Default\",\n","                99: \"Unknow\"\n","              }\n","\n","    mapping_expr = create_map([lit(x) for x in chain(*Status_code.items())])\n","    #fill missing value\n","    AIS_Dynamic_MSG = AIS_Dynamic_MSG.fillna(99.0,['status'])\n","    AIS_Dynamic_MSG = AIS_Dynamic_MSG.withColumn('status',mapping_expr[AIS_Dynamic_MSG['status']])\n","    \n","\n","    return AIS_Dynamic_MSG"]},{"cell_type":"code","execution_count":4,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"7795e17d-d4fb-4425-b579-66aae22b6c85","showTitle":false,"title":""}},"outputs":[],"source":["def static_report_vessel(df):\n","    \"\"\"\n","    This function retuns the dataframe with static ais messages\n","    \n","    \"\"\"\n","    def Vessel_type_mapping(x):\n","                if (x >=20 and x<30):\n","\n","                        return \"Wing In Ground\"\n","                elif (x==51):\n","\n","                        return \"search and rescue\"\n","                elif  (x==31):\n","\n","                        return \"Fishing Vessel\"\n","                elif(x==31 or x ==32 or x ==33):\n","\n","                        return 'Tug'\n","                elif ((x>33 and x <= 35) or (x>=50 and x <= 59 )):\n","\n","                        return \"Fishing Vessel\"\n","                elif(x == 36):\n","        \n","                        return 'Special Craft'\n","                elif (x == 37):\n","\n","                        return 'Pleasure Craft'\n","                elif (x == 36):\n","                        return 'Sailing Vessel'\n","                \n","                elif (x ==38 or x == 39):\n","        \n","                        return 'Reserved'    \n","                \n","                elif (x >=40 and x<50):\n","                \n","                        return 'High-Speed Craft' \n","                \n","                elif (x>=60 and x<70):\n","                \n","                        return 'Passenger Ship'\n","                        \n","                elif (x>=70 and x< 80):\n","                        \n","                        return 'Cargo'\n","                \n","                elif (x>=80 and x< 90):\n","        \n","        \n","                        return 'Tanker'\n","        \n","                elif (x>=90 and x< 100):\n","                        \n","                        return 'Other'\n","                else : \n","                        return 'Unknown'\n","                \n","            \n","\n","    # select the dynamics messages\n","    msg_type_filter = col('msg_type').isin([5,19,24,18])\n","    AIS_Static_MSG = df.filter(msg_type_filter) \n","    \n","    #calculate ship_length & ship_width\n","    AIS_Static_MSG = AIS_Static_MSG.withColumn(\"ship_area\",col('to_bow')+col('to_stern')*( col('to_starboard') + col('to_port')))\n","    \n","    #calculte the lengh of ship\n","    AIS_Static_MSG = AIS_Static_MSG.withColumn(\"ship_length\",col('to_bow')+col('to_stern'))\n","     \n","    #calculte the width of ship\n","    AIS_Static_MSG = AIS_Static_MSG.withColumn(\"ship_width\",col('to_starboard') + col('to_port'))\n","    AIS_Static_MSG = AIS_Static_MSG.select('mmsi','imo','callsign','shipname','ship_type','ship_area','ship_length','ship_width','draught','destination','eta')\n","    \n","    \n","    \n","    #regist dict mapping udf function\n","    Vessel_type_map = udf(lambda x: Vessel_type_mapping(x), StringType())\n","    #use 10086 to represent the missing ship type\n","    AIS_Static_MSG = AIS_Static_MSG.fillna(10086,['ship_type']) #replace missing value with same data type\n","    #dict mapping\n","    AIS_Static_MSG = AIS_Static_MSG.withColumn('ship_type',Vessel_type_map(col(\"ship_type\")))\n","    \n","    #fill missing value\n","    for index, column in enumerate(AIS_Static_MSG.columns):\n","        if column == 'destination' or column == 'callsign':\n","            AIS_Static_MSG = AIS_Static_MSG.na.fill(\"Unknown\")\n","            continue\n","        AIS_Static_MSG = AIS_Static_MSG.fillna(0,column)\n","    #drop duplicates\n","    \n","    AIS_Static_MSG = AIS_Static_MSG.dropDuplicates(['mmsi'])\n","\n","    return AIS_Static_MSG"]},{"cell_type":"code","execution_count":5,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"ef436d41-c927-41b4-b510-309efa6cd1c8","showTitle":false,"title":""}},"outputs":[],"source":["def GeoHash(df,grid_level):\n","    \"\"\"\n","    This function retun the dataframe with geohash gird\n","    \n","    \"\"\"\n","    # map geo location into geohash\n","    def convertCase(x,y):\n","        location_gird = pygeohash.encode(x,y,grid_level)\n","#         location  = pygeohash.decode(location_gird)\n","        #return location grid informat of latitude and longitude\n","        return location_gird \n","    # Converting function to UDF\n","    convertUDF = udf(convertCase, StringType())\n","\n","    #Geo hash mapping\n","    Ship_grid = df.withColumn('grid', convertUDF('lat', 'lon'))\n","    \n","    return Ship_grid\n","\n","def TimeWindow(df,seconds):\n","    \"\"\"\n","    This function retun the dataframe group by time widnow\n","    seconds: time to scan the geohash grid\n","    \n","    \"\"\"\n","    # create time window for every 5 min\n","    seconds_window = F.from_unixtime(F.unix_timestamp('time') - F.unix_timestamp('time') % seconds)\n","    Ship_grid = df.withColumn('time',col('timestamp').cast(TimestampType()))\n","    Ship_grid_time = Ship_grid.withColumn('5_minutes_window', seconds_window)\n","\n","\n","    #only keep the one ais msg for in every 5 min\n","    Ship_grid_time = Ship_grid_time.dropDuplicates(['5_minutes_window','mmsi'])\n","\n","    #calulate avg speed in every 5 min for each ships\n","    w = Window.partitionBy('mmsi','5_minutes_window')\n","    Ship_grid_time_avg_speed = Ship_grid_time.withColumn('avg_speed', F.mean('speed').over(w))\n","    return Ship_grid_time"]},{"cell_type":"code","execution_count":6,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"994d627b-9d53-43ea-8735-fb139b7abd18","showTitle":false,"title":""}},"outputs":[],"source":["def STS_meeting(dynamic_df,static_df):\n","    \"\"\"\n","    STS:\n","        2 or more different vessels meet in same grid （close to each other） and last more than 30 minutes\n","    \"\"\"\n","        \n","\n","    mid_point = udf(get_geo_mid,  ArrayType(FloatType()))\n","\n","    #   Get the duplicated ship info in per time window\n","    Ship_grid_distinct = dynamic_df.dropDuplicates(['5_minutes_window','mmsi','grid']).groupBy('5_minutes_window','grid','mmsi').count()\n","\n","    #  meeting for every 5 minute\n","    Ship_grid_distinct.createOrReplaceTempView(\"E\")\n","    Ship_grid_distinct.createOrReplaceTempView(\"D\")\n","  \n","    current = spark.sql(\"select * from E e, D d where e.5_minutes_window == d.5_minutes_window and e.grid == d.grid and e.mmsi != d.mmsi \").\\\n","    select('e.5_minutes_window','e.grid','e.mmsi',col('d.mmsi').alias('mmsi2'))\n","#            col('e.lat').alias('lat1'),col('e.lon').alias('lon1'),col('d.lat').alias('lat2'),col('d.lon').alias('lon2'))\n","\n","    # check the meeting last more than 5 min\n","    current.createOrReplaceTempView(\"current\")\n","    current.createOrReplaceTempView(\"future\")\n","    future = spark.sql(\"select * from current c, future f where f.5_minutes_window >c.5_minutes_window  and c.mmsi2 == f.mmsi2 and c.mmsi == f.mmsi\").\\\n","    select(col('c.5_minutes_window').alias('start'),  col('f.5_minutes_window').alias('5_min_later'), col('c.grid').alias('area'),'c.mmsi','c.mmsi2')\n","#            'c.lat1','c.lon1','c.lat2','c.lon2')\n","    # calculate meeting duration\n","    future = future.withColumn('duration',F.unix_timestamp('5_min_later') - F.unix_timestamp('start'))\n","    \n","    #meeting duration more than 0.5 hour\n","\n","    # only keep the time when meeting begin and end\n","    w = Window.partitionBy('c.mmsi','c.mmsi2')\n","    #single sts meeting\n","    sts_meeting = future.withColumn('start_time', F.min('start').over(w))\\\n","        .withColumn('end_time', F.max('5_min_later').over(w))\\\n","        .where(( F.col('5_min_later') == F.col('end_time') ) & (F.col('start_time') == F.col('start')))\n","    #cast the mmsi into array to union with meeting list\n","     \n","    #multi sts meeting\n","    #cast the mmsi into array to union with meeting list\n","    \n","    \n","    #transform mmsi in format of array to joint with other mmsi array e.g ‘【123241413】 【12312414124,3423425,34234234】’\n","    sts_meeting = sts_meeting.withColumn('mmsi_', array(col('mmsi')))\n","    sts_meeting = sts_meeting.withColumn('hour', hour('start_time'))\n","\n","\n","\n","    #only keep the qualified STSmeeting\n","    duration = col('duration')>=1800\n","    #Get the table of STS meeting and Multi STSmeeting \n","    meeting_list =     sts_meeting.filter(col('duration')>=1800).groupBy('mmsi','mmsi_','start', 'hour','area')\\\n","            .agg(collect_list('mmsi2').alias('meeting_with'))\\\n","            .withColumn('meeting', array_union(col(\"mmsi_\"),col('meeting_with')))\\\n","            .withColumn('meeting', array_sort(col('meeting'))).withColumnRenamed('area2','area')\n","    #generate unique meeting id for each STS meeting\n","    meeting_ID = meeting_list.dropDuplicates(['meeting']).withColumn(\"meeting_id\", monotonically_increasing_id()).select('meeting','meeting_id')\n","\n","\n","\n","\n","    #assin meeting id to every meeting \n","    meeting_list_with_id = meeting_list.join(meeting_ID,['meeting'])\n","    meeting_list_with_id = meeting_list_with_id.select('meeting','meeting_id',col('hour').alias('hour2'),col('mmsi').alias('mmsi2'),'area','start','meeting_with')\n","    \n","\n","    #augment info (dynamic & stastic) for each sts meeing\n","    dynamic_df = dynamic_df.withColumn('hour',hour('5_minutes_window'))\n","    \n","    Final_sts_meeting = meeting_list_with_id.join(dynamic_df, (dynamic_df.mmsi == meeting_list_with_id.mmsi2)  & (meeting_list_with_id.start == col('5_minutes_window')) & (dynamic_df.grid == meeting_list_with_id.area) & (dynamic_df.hour == meeting_list_with_id.hour2) ,'inner')\\\n","                               .join(static_df,['mmsi'],'left')\\\n","                               .drop('mmsi2')\n","    \n","    \n","\n","#     fill missing value\n","    for index, column in enumerate(Final_sts_meeting.columns):\n","            # fill null with \"Unknown\" for static info\n","            if column in (['callsign','shipname','ship_type','destination','eta']):\n","                Final_sts_meeting = Final_sts_meeting.na.fill(\"Unknown\")\n","                continue\n","            Final_sts_meeting = Final_sts_meeting.fillna(0,column)\n","    return Final_sts_meeting"]},{"cell_type":"code","execution_count":7,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"b92f1bfe-4d81-4c09-8fff-4a8ded1cb472","showTitle":false,"title":""}},"outputs":[],"source":["def output_file(df):\n","    \"\"\"  select the cols that are useful for visualization  \"\"\"\n","    \n","    \n","    #crate unique index for visualization on map e.g meetingid-mssi\n","    Final_sts_meeting = df.withColumn('mmsi',col('mmsi').cast(StringType()))\n","    Final_sts_meeting = Final_sts_meeting.withColumn('meeting_id',col('meeting_id').cast(StringType()))\n","    Final_sts_meeting = Final_sts_meeting.withColumn('name',concat(col(\"meeting_id\"),lit(\"-\"),col(\"mmsi\")))\n","    Final_sts_meeting = Final_sts_meeting.withColumn('min',minute('start'))\n","\n","\n","    \n","    #select the row for visualization\n","    output =  Final_sts_meeting.select(col('name'),col('shipname').alias('ship_name'),col(\"lat\").alias('latitude'),col(\"lon\").alias('longitude'),'speed','heading','course','status','ship_type','draught','destination','eta',col('ship_length').alias('length'),col('ship_width').alias('width'),'imo','meeting','meeting_id',col('start').alias('meeting_time'),col('hour'),'min','mmsi','meeting_with')\n","                                       \n","\n","    return output"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"e22b9f7d-77dc-4b01-9323-a39a633bd85d","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"},{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>"]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"","errorSummary":"Cancelled","errorTraceType":"html","metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["filelist = dbutils.fs.ls('/mnt/lsde/group12/input/')\n","out_put_dir = '/mnt/lsde/group12/output-minute-final/'\n","# dbutils.fs.rm(out_put_dir,recurse = True)\n","counter = []\n","#process data day by day \n","for file in filelist[:]:\n","    \n","    #get the date and generate output file path\n","    date= re.sub('-0','-',file[0].split('/')[-2])\n","    out_out_file_dir = out_put_dir+date\n","    \n","    df = spark.read.option(\"header\", \"false\").option(\"delimiter\", \",\").parquet(file[0])\n","    #get the vessel static data\n","    AIS_Static_MSG_df = static_report_vessel(df)\n","    #get the vessel dynamic data\n","    Ship_AIS_Dynamic_df = Ship_position_reports(df)\n","    #convert location in to grid\n","    Ship_grid = GeoHash(Ship_AIS_Dynamic_df,6)\n","    #group by data with time window \n","    Ship_grid_time =TimeWindow(Ship_grid,300)\n","    #caculate sts meeting\n","    Final_sts_meeting = STS_meeting(Ship_grid_time,AIS_Static_MSG_df)\n","    #output file\n","    output=  output_file(Final_sts_meeting)\n","\n","\n","\n","    #save file in format of json\n","    output.coalesce(1).write.partitionBy(\"meeting_time\").mode(\"Overwrite\").format('json').save(out_out_file_dir)\n","    file = dbutils.fs.ls('/mnt/lsde/group12/output-minute-final/'+date)\n","    path = '/mnt/lsde/group12/output-minute-final/'+date\n","    #delete redundant file output by Spark\n","    for fl in file[:]:\n","            #check the whether it is  json fil\n","        if 'hour'in fl[0].split('/')[-2]:\n","            for minute_ in dbutils.fs.ls(fl[0]):\n","                for i in  dbutils.fs.ls(minute_[0]):\n","                    #check  whether it is json file\n","                    if '.json'  not in i[0].split('/')[-1]:\n","                        dbutils.fs.rm(i[0],recurse = True)\n","    \n","                    else:\n","\n","                   #change the name of raw json file\n","                      hour_ = re.findall(r\"\\d+\\.?\\d*\",fl[0].split('/')[-2])\n","                   #delete the original directory\n","                      min_ = re.findall(r\"\\d+\\.?\\d*\",minute_[0].split('/')[-2])\n","                      dbutils.fs.mv(i[0], 'dbfs:'+out_put_dir+date+'/hour='+str(hour_[0])+'/ships-'+date+'-'+str(hour_[0])+'-'+min_[0].zfill(2)+'.json', True)\n","                dbutils.fs.rm(minute_[0])\n","    print(date+' is'+' ready')\n","                        "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":4},"notebookName":"Data transformation pipeline","notebookOrigID":2832058133161509,"widgets":{}},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.7"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}
